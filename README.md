# data_analysis_team2_dudnik
## Исследовательская задача: анализ данных (“бигдата”) из системы GLS 
О чем речь В системе GLS накапливаются данные о грузах и грузовых пространствах, о достигнутых метриках качества укладки. Есть гипотезы о том, как использовать эти данные для решения некоторых задач повышения продуктивности GLS. 

Предлагается в двух вариантах (на выбор студентов):

“базовый вариант” - сделать модель (и естественно “закодить”), которая будет предсказывать значение плотности укладки грузов в контейнере (грузовом пространстве). 

“развернутый вариант” предполагает выделить метрики геометрических характеристик грузов, на основе которых грузы можно было бы сортировать по контейнерам таким образом, чтобы плотность укладки с сортировкой была бы выше, чем без сортировки (отбор случайным образом); вариантом решения также может быть нейронная сеть обученная для выполнения данной задачи; Требования к результату В основном такие: описание математической постановки выбранной задачи описание выбранного метода решения задачи написан программный код, реализующий описанный метод приведена документация достаточно подробная (минимально необходимая) для самостоятельного запуска кода.
Приветствуется содержательная интерпретация полученного результата.
# Базовый вариант
## Инструкция по использованию
В данном репозитории есть несколько файлов: {main.py} {test.py} {Garpixx (1).ipynb}. 
В первом код алгоритм на питоне, которому на вход подается файл в формате JSON. Из него достаются и создаются необходимые для работы алгоритма данные и метрики, соответственно. На выходе получается json файл, необходимый для обучения наших моделей.
Во втором на вход подается json файл, для которого необходимо предсказать плотность укладки. Количество наборов коробок, для которых нужно предсказать плотность укладки, может быть неограниченым.
В третьем лежит jupiter ноутбук, с моделям и их точностью. К каждой модели выведены значения explained_variance_score (близкий аналог R2-score), среднеквадратичное отклонение, максимальную и среднюю абсолютную ошибку. Т.е оснвоные показатели качества модели. Наша рекомендация: пользоваться градиентным бустингом или случайным лесом (XGBoost и RandomForestRegressor, соответстветнно).
#
Для работы с моделями необходимо полученные, после выполнения {1} и {2} алгоритма, json файлы преоборазовать в csv формат. Сделать это можно через онлайн-конвертер. 
#
{garpixx.json} полученый полсе выполнения алгоритма {1}, на данных, которые нам предлагались. На них были обучены модели.
